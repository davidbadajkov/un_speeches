{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run import_packages.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgtvAJN_BObV"
   },
   "source": [
    "## Section 1 : Loading Various Data and Merging all the transcripts\n",
    "In our source code we create a dataframe with all the relevant information of the speeches in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVmheYEz8_Dv",
    "outputId": "290c4caf-dcbb-4ce7-d8f5-fcf034ad541e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codes/app/requirements.txt\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8093 entries, 0 to 8093\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Year        8093 non-null   object\n",
      " 1   Session     8093 non-null   object\n",
      " 2   Country     8093 non-null   object\n",
      " 3   Transcript  8093 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 316.1+ KB\n",
      "      Year Session Country                                         Transcript\n",
      "780   1976      31     NZL  b'Mr. President, it is my great pleasure to co...\n",
      "790   1976      31     QAT  b\"On behalf of the Minister for Foreign Affair...\n",
      "6075  2008      63     MEX  b'Allow me first of all to congratulate you, \\...\n",
      "5978  2008      63     BEN  b'I warmly \\r\\ncongratulate the President on h...\n",
      "5389  2004      59     VUT  b'I bring to this\\r\\ngathering a warm greeting...\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('https://s3grouparmenia.s3.eu-central-1.amazonaws.com/data/consolidated_transcripts.csv')\n",
    "%run consolidating_transcripts.py\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\envs\\mda_001\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Your version of xlrd is 1.2.0. In xlrd >= 2.0, only the xls format is supported. As a result, the openpyxl engine will be used if it is installed and the engine argument is not specified. Install openpyxl instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sdi = pd.read_excel('https://s3grouparmenia.s3.eu-central-1.amazonaws.com/data/SDI_data/SDI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOh0rwfLBH82"
   },
   "source": [
    "## Section 2 : Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NrOqckUEzsC"
   },
   "source": [
    "###  DataCleaners\n",
    "\n",
    "We created a class *DataCleaners* to clean and lemmatize the text, as well as remove stopwords with a custom dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pelRKoN47VbL",
    "outputId": "6ba6cbd9-5110-4cfb-fd88-57d376f038d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "['clean_text', 'lemmatizer', 'remove_stopwords']\n"
     ]
    }
   ],
   "source": [
    "import cleaners\n",
    "# custom methods\n",
    "print(dir(cleaners.DataCleaners)[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLywiKSIFFdx"
   },
   "source": [
    "Based on the topic modelling analysis we show below in Section 3, we found out that many words were repeated which were not very meaningful given the context of our analysis. Words such as 'united', 'nations', 'assembly', etc. Here we use a custom dictionary to extract them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7B_rbSFMGaTl"
   },
   "outputs": [],
   "source": [
    "custom_stopwords = ['united','nations','nation', 'international','society','organization','organizations','member','state',\n",
    "                                'relations','relation','global','charter','general','assembly','year','ago','/n','/t','/n/n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-KdvOFPFMU4"
   },
   "source": [
    "We now can pre-process the entire dataset using the DataCleaners class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell may take a while, alternatively run:\n",
    "# df = pd.read_csv('https://s3grouparmenia.s3.eu-central-1.amazonaws.com/data/consolidated_transcripts.csv')\n",
    "\n",
    "df['Transcript'] = df['Transcript'].apply(lambda x : cleaners.DataCleaners.clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transcript'] = df['Transcript'].apply(lambda x : cleaners.DataCleaners.remove_stopwords(str(x), custom_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Transcript'] = df['Transcript'].apply(lambda x : cleaners.DataCleaners.lemmatizer(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ym4x2zFMFTVy"
   },
   "outputs": [],
   "source": [
    "# Save the cleaned transcripts\n",
    "df.to_csv('cleaned_transcripts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KukdeDcUBgwd"
   },
   "source": [
    "## Section 3 : Extract Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-vHXykTF2Io"
   },
   "source": [
    "### N-grams\n",
    "We define a function to extract the top k ngrams. We decide to plot the 10 most frequent bigrams from our cleaned transcript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcHxe_FfVRhz"
   },
   "outputs": [],
   "source": [
    "def top_k_ngram(corpus, n = 3, k = 10):\n",
    "    vec = CountVectorizer(ngram_range=(n, n), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "9wtBymb9F0nq",
    "outputId": "28040e91-94ea-4f1e-8d6c-199e25cd9ee7"
   },
   "outputs": [],
   "source": [
    "common_words = top_k_ngram(df['Transcript'].values.astype('U'), 2, 10)\n",
    "df2 = pd.DataFrame(common_words, columns = ['bigram', 'count'])\n",
    "\n",
    "fig = go.Figure([go.Bar(x=df2['bigram'], y=df2['count'])])\n",
    "fig.update_layout(title=go.layout.Title(text=\"Top 10 bigrams in the text after removing stop words and lemmatization\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9ReOaswF_Av"
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSVJMB4WBk0P"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=3,                       \n",
    "                             stop_words='english',             \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  \n",
    "                             max_features=5000,          \n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(df['Transcript'].values.astype('U'))\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=8, # Number of topics\n",
    "                                      learning_method='online',\n",
    "                                      random_state=0,       \n",
    "                                      n_jobs = -1  # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FX-NF-hOW9b"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtOHirYpRwz7"
   },
   "outputs": [],
   "source": [
    "# Top 20 most frequent words from each topic found by LDA\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20)\n",
    "\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjMv1XvYQ_ii",
    "outputId": "b85507d9-f085-4e9f-b131-6032d532fe6b"
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = df_topic_keywords.T.columns\n",
    "print(topicnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lZJ0Dm3Q5xb"
   },
   "outputs": [],
   "source": [
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEGPG6y9M_FL",
    "outputId": "e3398181-783f-4134-8793-00cd2c9d45a0"
   },
   "outputs": [],
   "source": [
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "df_document_topic.reset_index(inplace=True)\n",
    "df_sent_topic= pd.merge(df, df_document_topic, left_index=True, right_index=True)\n",
    "df_sent_topic.drop('index', axis=1, inplace=True)\n",
    "\n",
    "df_topic_theme = df_sent_topic[['Transcript', 'dominant_topic']]\n",
    "#print(df_document_topic)\n",
    "\n",
    "print(df_topic_theme.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0XlVb36GHyY"
   },
   "outputs": [],
   "source": [
    "# We determined the topic names by looking at the top 30 most salient words provided by the LDA analysis.\n",
    "df_topic_theme['topic_name'] = np.nan\n",
    "df_topic_theme.info\n",
    "df_topic_theme.reset_index(inplace=True)\n",
    "for i in range(0,len(df_topic_theme)):\n",
    "    if df_topic_theme['dominant_topic'][i] == 0:\n",
    "        df_topic_theme['topic_name'][i] = 'development of africa'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 1:\n",
    "        df_topic_theme['topic_name'][i] = 'human rights'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 2:\n",
    "        df_topic_theme['topic_name'][i] = 'international security'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 3:\n",
    "        df_topic_theme['topic_name'][i] = 'nuclear politics'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 4:\n",
    "        df_topic_theme['topic_name'][i] = 'economic development'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 5:\n",
    "        df_topic_theme['topic_name'][i] = 'israel-palestine conflict'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 6:\n",
    "        df_topic_theme['topic_name'][i] = 'world peace'\n",
    "    elif df_topic_theme['dominant_topic'][i] == 7:\n",
    "        df_topic_theme['topic_name'][i] = 'sustainable development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYUpZ4W5GQD7"
   },
   "outputs": [],
   "source": [
    "# Merge the df_topic_theme with the main dataframe\n",
    "df1 = df.merge(df_topic_theme, on = 'Transcript', how = 'left')\n",
    "df = df1\n",
    "print(df.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18UqBYSISCQ2"
   },
   "source": [
    "## Section 4 : Extract Sentiment\n",
    "In this section we show how we extract the sentiment for each speech using the Textblob and the Vader lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOyRkLLsSRLe"
   },
   "outputs": [],
   "source": [
    "#We will extract the polarity level for each speech using Textblob\n",
    "# creating empty lists\n",
    "df['index'] = df.index\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "index = []\n",
    "\n",
    "# calculating the polarity and subjectivity level of each transcript\n",
    "for i in range(len(df)):\n",
    "    blob_polarity = TextBlob(str(df.Transcript.values[i])).sentiment[0]\n",
    "    blob_subjectivity = TextBlob(str(df.Transcript.values[i])).sentiment[1]\n",
    "    polarity.append(blob_polarity)\n",
    "    subjectivity.append(blob_subjectivity)\n",
    "    index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MT5c1fVvSjjK"
   },
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    'polarity' : polarity,\n",
    "    'subjectivity' : subjectivity,\n",
    "    'index' : index\n",
    "}\n",
    "scores_textblob = pd.DataFrame(mydict)\n",
    "df_sentiment = df.merge(scores_textblob,on='index')\n",
    "print(df_sentiment.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL7PBH_aSY0r"
   },
   "outputs": [],
   "source": [
    "#We will extract the polarity level for each speech using Vader lexicon\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "list_score = []\n",
    "list_index = []\n",
    "for i in range(0,len(df)):\n",
    "    score = vader.polarity_scores(str(df.Transcript[i]))\n",
    "    index = i\n",
    "    list_score.append(score)\n",
    "    list_index.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NftRhCwgS6l4"
   },
   "outputs": [],
   "source": [
    "# Convert the list of dicts into a DataFrame\n",
    "scores_vader = pd.DataFrame(list_score)\n",
    "scores_vader['index'] = list_index\n",
    "scores_vader.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vadsp1vNTJua"
   },
   "outputs": [],
   "source": [
    "# Join the DataFrames\n",
    "sentiment_df = df_sentiment.merge(scores_vader,on='index')\n",
    "print(sentiment_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJmLbzWKmn__"
   },
   "outputs": [],
   "source": [
    "df = sentiment_df\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazaTgpBax3Y"
   },
   "source": [
    "## Section 5 : Sustainable development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Du_7wGg1_8C"
   },
   "source": [
    "### Data consolidation\n",
    "We have [data](https://www.sustainabledevelopmentindex.org/methods) on the sustainable development index from 1990-2019. We now filter the UN speeches to only the ones that refer to sustainable development, and to match the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAcoWH9519lI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filter years between 1990-2019 to match time interval that measures SDI\n",
    "df2 = df[df['Year'] >= int('1990')]\n",
    "print(df2['Year'].min())\n",
    "# Keep only the observations related to sustainable development\n",
    "sus_df = df2[df2['topic_name'] == 'sustainable development']\n",
    "print(len(sus_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zsDNm9G19Z1"
   },
   "outputs": [],
   "source": [
    "# calculating the number of speeches per country that address sustainable development\n",
    "group_df = sus_df.groupby([\"Country\"])[sus_df.columns[6]].count()\n",
    "group_df = group_df.reset_index()\n",
    "group_df.rename(columns={sus_df.columns[6]:'count'}, inplace=True)\n",
    "print(group_df.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PhnGURl19Wz"
   },
   "outputs": [],
   "source": [
    "sdi2 = pd.melt(frame=sdi ,id_vars=[\"iso\", \"country\"],var_name=\"Year\",value_name=\"SDI\")\n",
    "sdi_l = sdi2[sdi2['Year'] != '2019']\n",
    "#sdi_l['Year_x']=sdi_l['Year_x'].astype(int)\n",
    "print(sdi_l['Year'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRqNPekK19Uj"
   },
   "outputs": [],
   "source": [
    "# calculating the mean SDI from 1990-2019 per country\n",
    "group_sdi = sdi_l.groupby([\"iso\"])[sdi_l.columns[3]].mean()\n",
    "group_sdi = group_sdi.reset_index()\n",
    "group_sdi.rename(columns={'SDI':'SDI_mean'}, inplace=True)\n",
    "print(group_sdi.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owEsrDiw19Ra"
   },
   "outputs": [],
   "source": [
    "# In order to visualize the SDI and the honesty ratio we define below, we download the geographical data using geopandas.\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "str(world['name']).lower()\n",
    "print(world.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxn3wUYk19OZ"
   },
   "outputs": [],
   "source": [
    "# merge geographical data with SDI data\n",
    "merge_df1 = pd.merge(world, group_sdi,how = 'left', left_on=[\"iso_a3\"], right_on=[\"iso\"]).drop(columns='iso')\n",
    "print(merge_df1.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSP8t5w219HG"
   },
   "outputs": [],
   "source": [
    "#merge with topic data\n",
    "map_df = pd.merge(merge_df1, group_df,how = 'left', left_on=[\"iso_a3\"], right_on=[\"Country\"]).drop(columns=\"Country\")\n",
    "print(map_df.sample(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKOcZgK13RZl"
   },
   "source": [
    "We now normalize the counts of the number of times each country talked about sustainable development.\n",
    "<br></br>\n",
    "$$\n",
    "\t\tspeech\\: count_{scaled} = \\frac{speech\\: count - speech\\: count_{min}}{speech\\:count_{max} - speech\\:count_{min}}\n",
    "\t\t$$\n",
    "<br></br>\n",
    "In addition, we defined an *Honesty Ratio* that measures the degree to which the countries actually implement what they talk about during the General Assembly.\n",
    "<br></br>\n",
    "\t\t$$\n",
    "\t\tHonesty\\: Ratio = \\frac{speech\\: count_{scaled}}{mean\\: SDI}\n",
    "\t\t$$\n",
    "\n",
    "Smaller values of the honesty ratio indicate better coordination from speech to implementation of sustainable development programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgo4DU533Unl"
   },
   "outputs": [],
   "source": [
    "#normalize count to 0-1 range\n",
    "diff1 = map_df[\"count\"]-map_df[\"count\"].min()\n",
    "diff2 = map_df[\"count\"].max()-map_df[\"count\"].min()\n",
    "map_df[\"normalized\"] = diff1 / diff2\n",
    "#computing honesty ratio\n",
    "map_df['index'] = map_df['normalized'] / map_df[\"SDI_mean\"]\n",
    "\n",
    "print(map_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGBoq2hN3ZxQ"
   },
   "outputs": [],
   "source": [
    "# Now that we have the data we need, we can make an interactive plot to compare the countries in terms of SDI and Honesty Ratio.\n",
    "fig = px.choropleth(map_df,\n",
    "                    geojson = map_df.geometry,\n",
    "                    locationmode = 'ISO-3',\n",
    "                    locations = map_df.iso_a3,\n",
    "                    color = 'index', ## to plot other variables, insert the name here\n",
    "                    color_continuous_scale = 'blues',\n",
    "                    projection = 'orthographic',\n",
    "                    hover_name = 'name',\n",
    "                    hover_data = ['SDI_mean', 'index'])\n",
    "fig.update_geos(fitbounds = 'locations', visible = True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJWqUO_K4nQw"
   },
   "outputs": [],
   "source": [
    "# We can also view the top/worst performers in Honesty Ratio terms by sorting the dataframe.\n",
    "#Top 10 worst coordination from speech to implement in improving sustainable environment\n",
    "map_df[['name','count','SDI_mean','index']].sort_values(by=['index'],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0E9jRTk3ct8"
   },
   "outputs": [],
   "source": [
    "#Top 10 best coordination\n",
    "map_df.loc[map_df['count'] > 1].sort_values(by = ['index']).dropna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbnrvHjfa4lS"
   },
   "source": [
    "## Section 6 : Israel-Palestine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgra5_2zmS7K"
   },
   "outputs": [],
   "source": [
    "print(df.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8ZNaDDZYrLd"
   },
   "outputs": [],
   "source": [
    "# Keep only speeches with the dominant topic of israel-palestine.\n",
    "df_palestina = df[df['dominant_topic'] == 5]\n",
    "\n",
    "# Normalize the data\n",
    "df_palestina['scaled_polarity'] = whiten(df_palestina['polarity'])\n",
    "df_palestina['scaled_subjectivity'] = whiten(df_palestina['subjectivity'])\n",
    "df_palestina['scaled_pos'] = whiten(df_palestina['pos'])\n",
    "df_palestina['scaled_neg'] = whiten(df_palestina['neg'])\n",
    "df_palestina['scaled_neu'] = whiten(df_palestina['neu'])\n",
    "\n",
    "# Our variables of interest for the clustering\n",
    "variables = ['scaled_polarity','scaled_subjectivity','scaled_pos','scaled_neg','scaled_neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxy73qfKYx6q"
   },
   "outputs": [],
   "source": [
    "# Create dataframes for interval of 10 years\n",
    "df_1970_1980 = df_palestina[df_palestina['Year'] < 1980]\n",
    "df_1980_1990 = df_palestina[(df_palestina['Year'] >= 1980) & (df_palestina['Year'] < 1990) ]\n",
    "df_1990_2000 = df_palestina[(df_palestina['Year'] >= 1990) & (df_palestina['Year'] < 2000) ]\n",
    "df_2000_2010 = df_palestina[(df_palestina['Year'] >= 2000) & (df_palestina['Year'] < 2010) ]\n",
    "df_2010_current = df_palestina[df_palestina['Year'] >= 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSV2h1TWdmGY"
   },
   "outputs": [],
   "source": [
    "# Take the mean for countries appearing multiple times\n",
    "grouped_df_1970_1980 = df_1970_1980.groupby(['Country'])[variables].mean()\n",
    "grouped_df_1980_1990 = df_1980_1990.groupby(['Country'])[variables].mean()\n",
    "grouped_df_1990_2000 = df_1990_2000.groupby(['Country'])[variables].mean()\n",
    "grouped_df_2000_2010 = df_2000_2010.groupby(['Country'])[variables].mean()\n",
    "grouped_df_2010_current = df_2010_current.groupby(['Country'])[variables].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YnXNi6idwcn"
   },
   "outputs": [],
   "source": [
    "# How to decide upon the number of clusters?\n",
    "# Declaring variables for use\n",
    "distortions = []\n",
    "num_clusters = range(1, 7)\n",
    "# Populating distortions for various clusters\n",
    "for i in num_clusters:\n",
    "    centroids, distortion = kmeans(grouped_df_1970_1980[variables], i)\n",
    "    distortions.append(distortion)\n",
    "# Plotting elbow plot data\n",
    "elbow_plot_data = pd.DataFrame({'num_clusters': num_clusters,\n",
    "'distortions': distortions})\n",
    "sns.lineplot(x='num_clusters', y='distortions',\n",
    "data = elbow_plot_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuEk1ydUd1NQ"
   },
   "outputs": [],
   "source": [
    "# Create the cluser labels\n",
    "cluster_centers,_ = kmeans(grouped_df_1970_1980[variables],3)\n",
    "grouped_df_1970_1980['cluster_labels_1'], _ = vq(grouped_df_1970_1980[variables],cluster_centers)\n",
    "# 1980 1990\n",
    "cluster_centers,_ = kmeans(grouped_df_1980_1990[variables],3)\n",
    "grouped_df_1980_1990['cluster_labels_2'], _ = vq(grouped_df_1980_1990[variables],cluster_centers)\n",
    "# 1990 2000\n",
    "cluster_centers,_ = kmeans(grouped_df_1990_2000[variables],3)\n",
    "grouped_df_1990_2000['cluster_labels_3'], _ = vq(grouped_df_1990_2000[variables],cluster_centers)\n",
    "# 2000 2010\n",
    "cluster_centers,_ = kmeans(grouped_df_2000_2010[variables],3)\n",
    "grouped_df_2000_2010['cluster_labels_4'], _ = vq(grouped_df_2000_2010[variables],cluster_centers)\n",
    "# 2010 \n",
    "cluster_centers,_ = kmeans(grouped_df_2010_current[variables],3)\n",
    "grouped_df_2010_current['cluster_labels_5'], _ = vq(grouped_df_2010_current[variables],cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOyeyZZxd-R4"
   },
   "outputs": [],
   "source": [
    "# How to interpret the cluster labels\n",
    "grouped_df_1970_1980.groupby('cluster_labels_1')[variables].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of Untitled7_with_daniel_16u47.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
